{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Askweet - Identifying questions in twitter streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humans are inquisitive by nature! The popularity of the internet has led to the creation of various portals to ask\n",
    "questions. Search engine are one of the most prominent go-to source for users seeking answers to their questions. The recent growth of social networks like Twitter, Facebook, Reddit has made it possible for people to direct questions towards their friends, followers, and other like-minded people.\n",
    "\n",
    "As an example, Twitter generates around 500 million tweets per day. Research has shown that roughly 10% of\n",
    "these are tweets are questions and many of them go unanswered. Imagine if we can develop an AI that can automatically classify tweets as questions and better yet, answer them directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering from the Twitter Firehose API\n",
    "Twitter Firehose is an API provided by Twitter that allows users to tweets real time. It is also possible to filter tweets based on certain criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print data\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "    stream.filter(track=['?','who', 'what', 'when', 'where', 'why', 'how', 'do', 'is', 'could', 'can', \"can't\", 'cant', 'would', \"wouldn't\", \"wouldnt\", 'should', \"shouldn't\", \"shouldnt\", 'did', 'will', 'has', 'have', \"won't\", 'does', 'wont', 'doesnt', \"doesn\\'t\", 'had', 'are'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data\n",
    "Once the data is captured in a text file, it is annotated with labels to create training data. Here is a sample set - \n",
    "\n",
    "tweet_id\t| tweet_original\t| is_question\t| is_answerable\n",
    "-----------  |    ---------      | ---------|--------\n",
    "29 | @TheGRAMMYs: When a band member leaves, what happens next? http://t.co/QOjFGFYdjv http://t.co/gR1a7WNyKg\t| yes\t| no\n",
    "30\t| @ThingsAKidSaid: When someone calls you ugly http://t.co/ewFB1RPENW\t| no |no\n",
    "34\t| RT @ambermakda: I have no words http://t.co/zjkPWUd7LZ\t| no\t| no \n",
    "35\t| So apparently the NHL replaced its refs with those from the NBA because that shouldn't have been a goaltenders interference penalty.\t| no | no \n",
    "24\t| Is David De Gea ready to extend his Manchester United contract?: ESPN FC's Craig Burley analyses Lukas Podolsk... http://t.co/eizdhI5xog\t| yes\t| yes\n",
    "\n",
    "As you can see, every tweet has two labels which identify whether it is a question and whether it is a machine answerable question. The second label is important if we want to consider only those tweets that can be answered automatically by machines. As a first step, building an AI to automatically answer factual questions (eg. What is Barrack Obama's height?) is a smaller problem than trying to answer questions that asks for opinions, or worse, rhetorical questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Feature Set\n",
    "The types of features extracted were broadly classified into 4 categories - \n",
    "## 1. Lexical features\n",
    "Given a tweet, it's unigrams, bigrams and trigrams were used as features.\n",
    "An n-gram is basically a set of n terms from a given tweet. Presence of ngrams such as \"what\", \"when will\" or \"why is US\" could imply that a tweet is indeed a question. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned data\n",
    "Lexical features needed data to be cleaned before unigrams, bigrams and trigrams were computed. During the process of cleaning, URLs, Twitter handles referring to usernames, and retweets were removed from the tweet\n",
    "\n",
    "tweet_id\t| tweet_original\t| is_question\t| is_answerable\n",
    "-----------  |    ---------      | ---------|--------\n",
    "29 |  When a band member leaves, what happens next? | yes\t| no\n",
    "30\t|  When someone calls you ugly | no |no\n",
    "34\t| I have no words \t| no\t| no \n",
    "35\t| So apparently the NHL replaced its refs with those from the NBA because that shouldn't have been a goaltenders interference penalty.\t| no | no \n",
    "24\t| Is David De Gea ready to extend his Manchester United contract?: ESPN FC's Craig Burley analyses Lukas Podolsk... \t| yes\t| yes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tweet format - \n",
      "{u'is_question': 0, u'is_answerable': 0, u'tags': [[u'RT', u'~', 0.9979], [u'@disneywords', u'@', 0.9991], [u':', u'~', 0.9803], [u'Many', u'A', 0.8671], [u'of', u'P', 0.9994], [u'the', u'D', 0.9996], [u'things', u'N', 0.9981], [u'that', u'P', 0.8976], [u'seem', u'V', 0.9083], [u'impossible', u'A', 0.9445], [u'now', u'R', 0.993], [u'will', u'V', 0.9931], [u'become', u'V', 0.9985], [u'realities', u'N', 0.9923], [u'tomorrow', u'N', 0.9558], [u'.', u',', 0.9968], [u'Walt', u'^', 0.9331], [u'Disney', u'^', 0.8178]]}\n",
      "\n",
      "\n",
      "Cleaned tweet -\n",
      "[u'rt', u':', u'mani', u'of', u'the', u'thing', u'that', u'seem', u'imposs', u'now', u'will', u'becom', u'realiti', u'tomorrow', u'.', u'walt', u'disney']\n",
      "\n",
      "\n",
      "Sample Unigram features - \n",
      "[u'yellow', u'four', u'otra', u'lord', u'digit', u'sehun', u'dell', u'prize', u'wednesday', u'solid']\n",
      "\n",
      "\n",
      "Sample Bigram features - \n",
      "[(u'my', u'final'), (u'hoodi', u'are'), (u'have', u'made'), (u'want', u'you'), (u'now', u'that'), (u'need', u'it'), (u'here', u'how'), (u'be', u'better'), (u'feel', u'at'), (u'this', u'site')]\n",
      "\n",
      "\n",
      "Sample Trigram features - \n",
      "[(u'8', u'year', u'old'), (u'explain', u'what', u\"you'r\"), (u'you', u\"didn't\", u'hear'), (u'at', u'all', u'.'), (u'two', u'more', u'week'), (u'want', u'them', u'to'), (u'own', u'one', u'of'), (u\"can't\", u'wait', u'for'), (u'you', u'deserv', u'all'), (u':', u'are', u'we')]\n",
      "\n",
      "The number of Unigram Features are 2994\n",
      "\n",
      "The number of Bigram Features are 6962\n",
      "\n",
      "The number of Trigram Features are 9393\n",
      "\n",
      "Total number of Lexical Features are 19349\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "class LexicalFeatures:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.featureIndex = defaultdict(int)\n",
    "        self.features = []\n",
    "        self.unigram_vocab = defaultdict(int)\n",
    "        self.bigram_vocab  = defaultdict(int)\n",
    "        self.trigram_vocab = defaultdict(int)\n",
    "        self.data = {}\n",
    "        self.get_data_from_file()\n",
    "\n",
    "    def get_data_from_file(self):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        infile = open(self.filename,\"r\")\n",
    "        self.data = json.load(infile)\n",
    "        infile.close()\n",
    "        print \"Input tweet format - \"\n",
    "        print self.data[\"5264\"]\n",
    "        print \"\\n\"\n",
    "        for ids in self.data:\n",
    "            tweet = self.data[ids][u'tags']\n",
    "            tweet_text = [ words[0] for words in tweet ]\n",
    "            # print tweet_text\n",
    "            clean_tweet = ' '.join(tweet_text)\n",
    "            regex_form = '^rt\\s+|@\\w+:*|https?://[\\w\\.\\/]*'\n",
    "            clean_tweet = re.sub(regex_form, '', clean_tweet)\n",
    "            clean_tweet = [stemmer.stem(x) for x in clean_tweet.split()]\n",
    "            if ids == \"5264\":\n",
    "                print \"Cleaned tweet -\"\n",
    "                print clean_tweet\n",
    "                print \"\\n\"\n",
    "                \n",
    "            for item in clean_tweet:\n",
    "                self.unigram_vocab[item] += 1\n",
    "\n",
    "            for item in list(nltk.bigrams(clean_tweet)):\n",
    "                self.bigram_vocab[item] += 1\n",
    "\n",
    "            for item in list(nltk.trigrams(clean_tweet)):\n",
    "                self.trigram_vocab[item] += 1\n",
    "            \n",
    "        # Use unigrams only if they occur 5 or more times\n",
    "        temp = [ k for k,v in self.unigram_vocab.iteritems() if v >= 5 ]\n",
    "        self.unigram_vocab = temp\n",
    "\n",
    "        temp = [ k for k,v in self.bigram_vocab.iteritems() if v >= 5 ]\n",
    "        self.bigram_vocab = temp\n",
    "\n",
    "        temp = [ k for k,v in self.trigram_vocab.iteritems() if v >= 3 ]\n",
    "        self.trigram_vocab = temp\n",
    "        print \"Sample Unigram features - \"\n",
    "        print self.unigram_vocab[0:10]\n",
    "        print \"\\n\"\n",
    "        print \"Sample Bigram features - \"\n",
    "        print self.bigram_vocab[0:10]\n",
    "        print \"\\n\"\n",
    "        print \"Sample Trigram features - \"\n",
    "        print self.trigram_vocab[0:10]\n",
    "        self.features = self.unigram_vocab + self.bigram_vocab + self.trigram_vocab\n",
    "\n",
    "        print \"\\nThe number of Unigram Features are %s\" % (len(self.unigram_vocab))\n",
    "        print \"\\nThe number of Bigram Features are %s\" % (len(self.bigram_vocab))\n",
    "        print \"\\nThe number of Trigram Features are %s\" % (len(self.trigram_vocab))\n",
    "        print \"\\nTotal number of Lexical Features are %s\" % (len(self.features))\n",
    "\n",
    "        for index, item in enumerate(self.features):\n",
    "            self.featureIndex[item] = index\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    inputFile = \"tweet_tags_15k.json\"\n",
    "    trainData = LexicalFeatures(inputFile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Meta features\n",
    "\n",
    "These features describe the characteristics of the tweet. 5 different meta features were used - length of the tweet, number of words in the tweet, whether the tweet has a URL, whether the tweet mentions other users and the coverage of the words. Coverage corresponds to the number of unique features it has with respect to the dictionary of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureVectors:\n",
    "    def __init__(self):\n",
    "        self.featureVectors = defaultdict()\n",
    "\n",
    "    def compute_features(self, trainData):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        # pre save the sets of ngram\n",
    "        unigram_vocab_set = set(trainData.unigram_vocab)\n",
    "        bigram_vocab_set = set(trainData.bigram_vocab)\n",
    "        trigram_vocab_set = set(trainData.trigram_vocab)\n",
    "\n",
    "        for ids in trainData.data:\n",
    "            # print ids\n",
    "            features = [0]*len(trainData.features)\n",
    "            # meta features\n",
    "            length, number, has_url, has_mention, coverage = (0,)*5\n",
    "\n",
    "            #get the tweet\n",
    "            tweet = trainData.data[ids][u'tags']\n",
    "            tweet_is_question = trainData.data[ids][u'is_question']\n",
    "            tweet_is_answerable = trainData.data[ids][u'is_answerable']\n",
    "           \n",
    "            #get the tags\n",
    "            temp = set([x[2] for x in tweet])\n",
    "            #check for url and mentions\n",
    "            if 'U' in temp:\n",
    "                has_url = 1\n",
    "            if '@' in temp:\n",
    "                has_mention = 1\n",
    "\n",
    "            #compute len of tweet and number of words\n",
    "            length = sum([len(word[0]) for word in tweet])\n",
    "            number = len(tweet)\n",
    "\n",
    "            #process tweet for coverage and other Lexical features\n",
    "            tweet_text = [ words[0] for words in tweet ]\n",
    "            clean_tweet = ' '.join(tweet_text)\n",
    "            regex_form = '^rt\\s+|@\\w+:*|https?://[\\w\\.\\/]*'\n",
    "            clean_tweet = re.sub(regex_form, '', clean_tweet)\n",
    "            clean_tweet = [stemmer.stem(x) for x in clean_tweet.split()]\n",
    "\n",
    "            #compute coverage\n",
    "            coverage = len(set(clean_tweet)) / len(trainData.unigram_vocab)\n",
    "            #compute lexical features\n",
    "            for item in clean_tweet:\n",
    "                if item in unigram_vocab_set:\n",
    "                    features[trainData.featureIndex[item]] = 1\n",
    "\n",
    "            for item in list(nltk.bigrams(clean_tweet)):\n",
    "                if item in trigram_vocab_set:\n",
    "                    features[trainData.featureIndex[item]] = 1\n",
    "\n",
    "            for item in list(nltk.trigrams(clean_tweet)):\n",
    "                if item in trigram_vocab_set:\n",
    "                    features[trainData.featureIndex[item]] = 1\n",
    "\n",
    "            #append the lexical and meta features\n",
    "            lex_meta_features = features + [length, number, has_url, has_mention, coverage]\n",
    "            self.featureVectors[ids] = {'features':lex_meta_features,'is_question':tweet_is_question, 'is_answerable':tweet_is_answerable}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. POS Features\n",
    "\n",
    "These features compose of unigrams, bigrams and trigrams computed over part-of-speech tags. Since POS tags don't attach to particular words, POS features also identify context. Since the characteristics of tweets are slightly different from normal Engilish sentences, a specialized POS tagger for tweets developed by CMU was used - http://www.ark.cs.cmu.edu/TweetNLP/ The tagger uses seperate tags for URLs, hashtags, and also has specific tags for known shortforms (eg. ikr - i know right).\n",
    "Once the tags are identified for the tweet, unigrams bigrams and trigrams are computed over these tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img.png\" style=\"max-width:80%; \">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of CMU tagger\n",
      "[('I', 'O', 0.9972), ('cut', 'V', 0.9982), ('my', 'D', 0.9987), ('mouth', 'N', 0.9968), ('man', 'N', 0.6624), ('it', 'O', 0.9847), ('hurt', 'V', 0.9871), ('so', 'R', 0.9597), ('bad', 'A', 0.9412), ('when', 'R', 0.9858), ('I', 'O', 0.9981), ('eat', 'V', 0.9996), ('something', 'N', 0.9918)]\n",
      "Sample POS Unigram features - \n",
      "['!', '#', '$']\n",
      "\n",
      "\n",
      "Sample Bigram features - \n",
      "[('R', ','), ('U', 'E'), ('$', '&')]\n",
      "\n",
      "\n",
      "Sample Trigram features - \n",
      "[('X', ',', '^'), ('U', 'D', 'O'), ('P', 'N', 'T')]\n",
      "\n",
      "The number of POS Unigram Features are 24\n",
      "\n",
      "The number of POS Bigram Features are 340\n",
      "\n",
      "The number of POS Trigram Features are 1613\n",
      "\n",
      "Total number of POS Features are 1977\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import CMUTweetTagger\n",
    "import json\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "class TweetPOS_LM:\n",
    "    def __init__(self):\n",
    "        self.trainFile = 'Train1.csv'\n",
    "        self.tagFile = 'tweet_tags_15k.json'\n",
    "        self.featureFile = 'postags_features_15k.json'\n",
    "        self.tweet_id = []\n",
    "        self.tweet_original = []\n",
    "        self.is_question = []\n",
    "        self.is_answerable = []\n",
    "        self.tweet_unigram = defaultdict(set)\n",
    "        self.tweet_bigram = defaultdict(set)\n",
    "        self.tweet_trigram = defaultdict(set)\n",
    "        self.tweet_feature_list = []\n",
    "        self.tweet_features = {}\n",
    "\n",
    "    # read the file containing tweets\n",
    "    def readTweets(self):\n",
    "        with open(self.trainFile, 'rb') as csvfile:\n",
    "            tweetreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            header = tweetreader.next()\n",
    "            for row in tweetreader:\n",
    "                self.tweet_id.append(row[0])\n",
    "                self.tweet_original.append(row[1])\n",
    "                self.is_question.append(row[2])\n",
    "                self.is_answerable.append(row[3])\n",
    "\n",
    "    # This function computes the POS feature set\n",
    "    def pos_tagger(self):\n",
    "        tweets = []\n",
    "        for tw in self.tweet_original:\n",
    "            try:\n",
    "                tw = tw.decode('unicode_escape').encode('ascii','ignore')\n",
    "            except:\n",
    "                tw = re.sub(r'\\\\+', '', tw)\n",
    "                tw = tw.decode('unicode_escape').encode('ascii','ignore')\n",
    "            tweets.append(tw)\n",
    "        \n",
    "        pos_unig = []\n",
    "        pos_big = []\n",
    "        pos_trig = []\n",
    "        # tag tweets using python version of CMU Tweet NLP\n",
    "        sent_tags = CMUTweetTagger.runtagger_parse(tweets)\n",
    "        # fil_tweet = open('tweet_tags.json','w')\n",
    "        \n",
    "        print \"Result of CMU tagger\"\n",
    "        print sent_tags[5264]\n",
    "        i = 0\n",
    "        for sent in sent_tags:\n",
    "            unigrams = set([tag_tuple[1] for tag_tuple in sent])\n",
    "            bigrams = set(nltk.bigrams(unigrams))\n",
    "            trigrams = set(nltk.trigrams(unigrams))\n",
    "            self.tweet_unigram[self.tweet_id[i]] = unigrams\n",
    "            self.tweet_bigram[self.tweet_id[i]] = bigrams\n",
    "            self.tweet_trigram[self.tweet_id[i]] = trigrams\n",
    "            \n",
    "            pos_unig.extend(list(unigrams))\n",
    "            pos_big.extend(list(bigrams))\n",
    "            pos_trig.extend(list(trigrams))\n",
    "            i += 1\n",
    "        \n",
    "        pos_unig = list(set(pos_unig))\n",
    "        pos_big= list(set(pos_big))\n",
    "        pos_trig = list(set(pos_trig))\n",
    "        print \"Sample POS Unigram features - \"\n",
    "        print pos_unig[0:3]\n",
    "        print \"\\n\"\n",
    "        print \"Sample Bigram features - \"\n",
    "        print pos_big[0:3]\n",
    "        print \"\\n\"\n",
    "        print \"Sample Trigram features - \"\n",
    "        print pos_trig[0:3]\n",
    "        print \"\\nThe number of POS Unigram Features are %s\" % (len(pos_unig))\n",
    "        print \"\\nThe number of POS Bigram Features are %s\" % (len(pos_big))\n",
    "        print \"\\nThe number of POS Trigram Features are %s\" % (len(pos_trig))\n",
    "        self.tweet_feature_list = set(pos_unig + pos_big + pos_trig)\n",
    "        print \"\\nTotal number of POS Features are %s\" % (len(self.tweet_feature_list))\n",
    "\n",
    "    # This function creates a feature vector for every tweet\n",
    "    def get_features(self):\n",
    "        feature_index = {}\n",
    "        i = 0\n",
    "        for feature in self.tweet_feature_list:\n",
    "            feature_index[feature] = i\n",
    "            i += 1\n",
    "        i = 0\n",
    "        for tweet in self.tweet_id:\n",
    "            # feature_dict = {}\n",
    "            features = [0,]*len(feature_index)\n",
    "            for unigram in self.tweet_unigram[tweet]:\n",
    "                features[feature_index[unigram]] = 1\n",
    "            for bigram in self.tweet_bigram[tweet]:\n",
    "                features[feature_index[bigram]] = 1\n",
    "            for trigram in self.tweet_trigram[tweet]:\n",
    "                features[feature_index[trigram]] = 1\n",
    "            is_question = 1 if self.is_question[i] == \"yes\" else 0\n",
    "            is_anserable = 1 if self.is_answerable[i] == \"yes\" else 0\n",
    "            self.tweet_features[tweet] = {\"features\":features, \"is_question\":is_question, \"is_answerable\":is_anserable}\n",
    "            i += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tp = TweetPOS_LM()\n",
    "    tp.readTweets()\n",
    "    tp.pos_tagger()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WordNet features\n",
    "\n",
    "The last category of features are WordNet features which are features derived from word relationships. WordNet is a database of words containing a semantic lexicon for the English language that organizes words into groups called synsets (i.e., synonym sets). Synonyms and hypernyms of every word already in the tweet are extracted from the WordNet database and incorporated as a feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - Binormal Seperation\n",
    "\n",
    "The total size of the feature is roughly 22,000 features which means that every tweet is now represented by a vector of 22,000 dimensions! The number of tweets that we are working with is around 15,000+ tweets which makes it very sparse and it is not possible to get good classification results. This is expected given the huge and open vocabulary in Twitter. We adopted a state-of-the-art feature selection method named Bi-Normal Separation (BNS) for feature selection as it outperforms other well-known metric such as Information Gain and Chi-distance, especially when the dataset is very skewed.\n",
    "\n",
    "More information on bi-normal separation can be found here - http://www.hpl.hp.com/techreports/2007/HPL-2007-32R1.pdf. It is calculated using the formula - \n",
    "Bi-Normal Separation (BNS): | F-1(tpr) – F-1(fpr) | \n",
    "\n",
    "where\n",
    "* tp = number of positive tweets containing a feature,\n",
    "* fp = number of negative tweets containing word,\n",
    "* fn = pos – tp,\n",
    "* tn = neg – fp,\n",
    "* tpr=P(word | positive class) = tp/pos,\n",
    "* fpr=P(word | negative class) = fp/neg, \n",
    "* F-1 is the inverse Normal cumulative distribution function, as commonly available from statistical tables. \n",
    "\n",
    "Using BNS, all the features are ranked, higher the BNS value, better is the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.stats import norm\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def load_features_and_labels(filename):\n",
    "    print \"\\nStage: Load Features\"\n",
    "    #Feature1\n",
    "    infile1 = open(filename,\"r\")\n",
    "    feature1 = json.load(infile1)\n",
    "    infile1.close()\n",
    "    return feature1\n",
    "\n",
    "\n",
    "##### Input feature_dict is a dictionary where the keys are tweet IDs and the value is a tuple/list of feature vector, label\n",
    "##### Output is bns_features which is a dictionary of feature id (just an index number) and the BNS value.\n",
    "def count_metrics(feature_dict, label_type, bns_filename):\n",
    "    vals = feature_dict.values()\n",
    "    tweet_features = [f_tuple[\"features\"] for f_tuple in vals]\n",
    "    tweet_labels = [f_tuple[label_type] for f_tuple in vals]\n",
    "    num_features = len(tweet_features[0])\n",
    "    label_counts = Counter(tweet_labels)\n",
    "    pos = label_counts[1]\n",
    "    neg = label_counts[0]\n",
    "    print 'num_features',num_features\n",
    "    print 'pos',pos\n",
    "    print 'neg',neg\n",
    "\n",
    "    true_pos = defaultdict(int)\n",
    "    true_neg = defaultdict(int)\n",
    "    false_pos = defaultdict(int)\n",
    "    false_neg = defaultdict(int)\n",
    "\n",
    "    true_pos_rate = {}\n",
    "    false_pos_rate = {}\n",
    "\n",
    "    np_labels = np.array(tweet_labels)\n",
    "    for i in xrange(num_features):\n",
    "        vals = [features[i] for features in tweet_features]\n",
    "        # print vals\n",
    "        np_vals = np.array(vals)\n",
    "        true_pos[i] = np.sum(np.logical_and(np_vals, np_labels))\n",
    "        false_pos[i] = np.sum(np.logical_and(np_vals, np.logical_not(np_labels)))\n",
    "        true_pos_rate[i] = float(true_pos[i])/pos\n",
    "        false_pos_rate[i] = float(false_pos[i])/neg\n",
    "\n",
    "    bns_features = {}\n",
    "    for i in xrange(num_features):\n",
    "        if i%1000 == 0:\n",
    "            print 'Calculating',i,'of',num_features,'...'\n",
    "        print 'generating bns_features',i,'of',num_features,'...'\n",
    "        if true_pos_rate[i] != 0 and false_pos_rate[i] != 0 and true_pos_rate[i] != 1 and false_pos_rate[i] != 1:\n",
    "            bns_features[i] = abs(norm.ppf(true_pos_rate[i]) - norm.ppf(false_pos_rate[i]))\n",
    "\n",
    "    top_features = sorted(bns_features, key=bns_features.get, reverse=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features = load_features_and_labels(\"lexical_tags_15k.json\")\n",
    "    print 'getting bns scores for is_question...'\n",
    "    label_type = \"is_question\" \n",
    "    bns_filename = \"lexical_bns_scores_\"+label_type+\".json\"\n",
    "    count_metrics(features, label_type, bns_filename)\n",
    "    print 'getting bns scores for is_answerable...'\n",
    "    label_type = \"is_answerable\" \n",
    "    bns_filename = \"lexical_bns_scores_\"+label_type+\".json\"\n",
    "    count_metrics(features, label_type, bns_filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. The optimal hyperplane is the one that achieves the maximum seperation between the two classes. Since classifying tweets is a binary classification problem, we use a linear SVM.\n",
    "\n",
    "<img src=\"img2.png\" style=\"max-width:80%; \">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "import csv\n",
    "import numpy\n",
    "\n",
    "def run_svm(feature, labels):\n",
    "    '''Fitting a classifier using SVM Regression and predicting the values\n",
    "            using linear kernel and perform five fold cross validation\n",
    "    '''\n",
    "    print \"\\nStage: SVM\"\n",
    "\n",
    "    X = feature\n",
    "    Y = labels\n",
    "    \n",
    "    clf = svm.SVC(kernel='linear', C=1)\n",
    "    scores = cross_validation.cross_val_score(clf, X, Y, cv=5)\n",
    "    accuracy = scores.mean()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def load_json(filename):\n",
    "    print \"\\nStage: Load Features\"\n",
    "    #Feature1\n",
    "    json_file = open(filename,\"r\")\n",
    "    new_json = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "    return new_json\n",
    "\n",
    "def select_top_k_features(feature_dict, k, label_type,bns_scores):\n",
    "    top_features = bns_scores[:k]\n",
    "    tweet_features = []\n",
    "    tweet_labels = []\n",
    "    for tweet_id,features in feature_dict.iteritems():\n",
    "        feature_list_n = numpy.array(features[\"features\"])\n",
    "        feature_list_k = feature_list_n[top_features]\n",
    "        tweet_features.append(feature_list_k)\n",
    "        tweet_labels.append(features[label_type])\n",
    "\n",
    "    return (tweet_features, tweet_labels)\n",
    "\n",
    "def run_svm_for_bns_features(feature_type):\n",
    "    feature_dict = load_json(feature_type+\"_features_15k.json\")\n",
    "    label_type_list = [\"is_question\",\"is_answerable\"]\n",
    "    result = {}\n",
    "\n",
    "    print \"Running for feature type\",feature_type\n",
    "\n",
    "    for label_type in label_type_list:\n",
    "        print \"Running svm for label:\",label_type,\"\\n\\n\"\n",
    "        bns_scores = load_json(feature_type+\"_bns_scores_\"+label_type+\".json\")\n",
    "        result[label_type] = {}\n",
    "\n",
    "        k =len(bns_scores)\n",
    "        # print bns_scores\n",
    "        print 'k:'+str(k)\n",
    "        (features_k, labels_k) = select_top_k_features(feature_dict, k, label_type, bns_scores)\n",
    "        accuracy = run_svm(features_k, labels_k)\n",
    "        print 'accuracy:'+str(accuracy)\n",
    "        result[label_type][k] = accuracy\n",
    "\n",
    "        for k in xrange(200,len(bns_scores),200):\n",
    "            print 'k:'+str(k)\n",
    "            (features_k, labels_k) = select_top_k_features(feature_dict, k, label_type, bns_scores)\n",
    "            accuracy = run_svm(features_k, labels_k)\n",
    "            print 'accuracy:'+str(accuracy)\n",
    "            result[label_type][k] = accuracy\n",
    "\n",
    "    return result\n",
    "\n",
    "def run_svm_for_all_features(feature_type):\n",
    "\n",
    "    feature_dict = load_json(feature_type+\"_features_15k.json\")\n",
    "    label_type_list = [\"is_question\",\"is_answerable\"]\n",
    "    result = {}\n",
    "\n",
    "    for label_type in label_type_list:\n",
    "        tweet_features = []\n",
    "        tweet_labels = []\n",
    "        for tweet_id,features in feature_dict.iteritems():\n",
    "            feature_list_n = numpy.array(features[\"features\"])\n",
    "            tweet_features.append(feature_list_n)\n",
    "            tweet_labels.append(features[label_type])\n",
    "\n",
    "        result[label_type] = {}\n",
    "        accuracy = run_svm(tweet_features, tweet_labels)\n",
    "        result[label_type] = accuracy\n",
    "    return result\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "For every feature type, SVM was run twice, once to classify as 'isQuestion' and the second time to classify as 'isAnswerable'. The first time, the classifiers were run on the raw features without feature selection. The second time, the classifiers were run by by selecting the top 'k' features in each category.\n",
    "\n",
    "<img src=\"img3.png\">\n",
    "\n",
    "<img src=\"img4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
